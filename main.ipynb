{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ec5bbc-8117-4d0d-8716-2e7dfad598b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T10:15:05.309873Z",
     "iopub.status.busy": "2024-04-08T10:15:05.309560Z",
     "iopub.status.idle": "2024-04-08T10:15:07.124055Z",
     "shell.execute_reply": "2024-04-08T10:15:07.123701Z",
     "shell.execute_reply.started": "2024-04-08T10:15:05.309842Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import timeit\n",
    "import random\n",
    "import shutil\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from datetime import datetime\n",
    "from typing import Any, Callable, Optional, Sequence, Union\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import weight_norm\n",
    "import torch.utils.data\n",
    "import torchmetrics\n",
    "\n",
    "from monai.config import DtypeLike\n",
    "from monai.data.image_reader import ImageReader\n",
    "from monai.data import ImageDataset\n",
    "from monai.transforms import Compose, RandAffine, AddChannel, ScaleIntensity, RandFlip, Resize, NormalizeIntensity, ToTensor, SpatialPad\n",
    "\n",
    "iterations = [1, 5] # 1\n",
    "epochs = 100 # 1\n",
    "batch_size = 8 # 2\n",
    "num_workers = 16 # 0\n",
    "\n",
    "module_names = ['GOTDNet']\n",
    "model_names = ['GOTDNet']\n",
    "\n",
    "model_dir = 'models'\n",
    "for  module_name in module_names:\n",
    "    exec(f'from {model_dir}.{module_name} import *')\n",
    "\n",
    "dataset_dir = '../Total_Datasets/Datasets-TAO/231002_Treatment_Proposed_Dataset_v5'\n",
    "label_file = 'Treatment_Label_v0.5_230913.csv'\n",
    "\n",
    "devices = [0,1]\n",
    "Metrics = ['Experient Time', 'Train Time','Iteration', 'Model Name', 'Loss', 'AUROC', 'AUPRC', 'Accuracy', 'F1 Score', 'Sensitivity', 'Specificity', 'Precision', 'Threshold', 'Params', 'Training Time (s)', 'Test Time (s)', 'Best_Epoch','DIR']\n",
    "in_channels = 1\n",
    "num_classes = 2\n",
    "\n",
    "optimizer = 'adamw'\n",
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "optim_args = {'optimizer': optimizer, 'lr': lr, 'momentum': momentum, 'weight_decay': weight_decay}\n",
    "\n",
    "lr_scheduler = 'CosineAnnealingLR'\n",
    "T_max = epochs\n",
    "T_0 = 50\n",
    "eta_min = 1e-6\n",
    "lr_scheduler_args = {'lr_scheduler': lr_scheduler, 'T_max': T_max, 'T_0': T_0, 'eta_min': eta_min}\n",
    "\n",
    "loss_function = 'FocalLoss'\n",
    "reduction = 'mean'\n",
    "gamma = 2.0\n",
    "weight = None\n",
    "loss_function_args = {'loss_function': loss_function, 'reduction': reduction, 'gamma': gamma, 'weight': weight}\n",
    "\n",
    "save_log = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9735f080-655c-4f15-8358-9fcd46791275",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T10:15:07.125152Z",
     "iopub.status.busy": "2024-04-08T10:15:07.124917Z",
     "iopub.status.idle": "2024-04-08T10:15:07.239624Z",
     "shell.execute_reply": "2024-04-08T10:15:07.239256Z",
     "shell.execute_reply.started": "2024-04-08T10:15:07.125139Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, criterion, train_loader, device):\n",
    "    model.train()\n",
    "    train_losses = AverageMeter()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # print(f'Step {i+1}/{len(train_loader)}', end=' ')\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        input, target, _ = batch\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)#.float()\n",
    "        output = model(input) \n",
    "        output = nn.Softmax(dim=1)(output)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.update(loss.detach().cpu().numpy(),input.shape[0])\n",
    "    train_losses = round(float(train_losses.avg),6)\n",
    "    return train_losses  \n",
    "\n",
    "def infer(model, criterion, valid_loader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():     \n",
    "        for i, batch in enumerate(valid_loader):\n",
    "            # print(f'Step {i+1}/{len(valid_loader)}', end=' ')\n",
    "            input, target, sample_index = batch\n",
    "            target = target.to(device)#.float()\n",
    "            output = model(input)\n",
    "            output = nn.Softmax(dim=1)(output)\n",
    "            if i==0:\n",
    "                targets = target.to('cpu')\n",
    "                outputs = output.to('cpu')\n",
    "                sample_indexes = sample_index\n",
    "            else:\n",
    "                targets = torch.cat((targets, target.to('cpu')))\n",
    "                outputs = torch.cat((outputs, output.to('cpu')), axis=0)\n",
    "                sample_indexes = torch.cat((sample_indexes, sample_index))\n",
    "    return outputs, targets, sample_indexes\n",
    "\n",
    "def count_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return \"{:4.2f} M\".format( num_params/1000000 )\n",
    "\n",
    "def copy_sourcefile(output_dir, src_dir = 'src' ):    \n",
    "    import os \n",
    "    import shutil\n",
    "    import glob \n",
    "    source_dir = os.path.join(output_dir, src_dir)\n",
    "\n",
    "    os.makedirs(source_dir, exist_ok=True)\n",
    "    org_files1 = os.path.join('./', '*.py' )\n",
    "    org_files2 = os.path.join('./', '*.sh' )\n",
    "    org_files3 = os.path.join('./', '*.ipynb' )\n",
    "    org_files4 = os.path.join('./', '*.txt' )\n",
    "    org_files5 = os.path.join('./', '*.json' )    \n",
    "    files =[]\n",
    "    files = glob.glob(org_files1 )\n",
    "    files += glob.glob(org_files2  )\n",
    "    files += glob.glob(org_files3  )\n",
    "    files += glob.glob(org_files4  ) \n",
    "    files += glob.glob(org_files5  )     \n",
    "\n",
    "    # print(\"COPY source to output/source dir \", files)\n",
    "    tgt_files = os.path.join( source_dir, '.' )\n",
    "    for i, file in enumerate(files):\n",
    "        shutil.copy(file, tgt_files)\n",
    "\n",
    "def init_weights(m, mean=0.0, std=0.01):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        m.weight.data.normal_(mean, std)\n",
    "\n",
    "def save_checkpoint(args, model, optimizer, optim_args, epoch, loss, output_dir, file_path):\n",
    "    state = {\n",
    "        'args'           : args,\n",
    "        'model_state'    : model.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict(),\n",
    "        'optim_args'     : optim_args,\n",
    "        'epoch'          : epoch,\n",
    "        'val_loss'       : loss,\n",
    "        }\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    torch.save(state, file_path)   \n",
    "\n",
    "def load_checkpoint(model, optimizer, file_path):\n",
    "    import os\n",
    "    import torch\n",
    "    dst = f'cuda:{torch.cuda.current_device()}'\n",
    "    checkpoint = torch.load(file_path, map_location=dst)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "\n",
    "def configure_optimizer(model, optim_args) :\n",
    "    import torch.optim as optim\n",
    "    if optim_args['optimizer'].lower() == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(),     lr=optim_args['lr'], momentum=optim_args['momentum'])\n",
    "    elif optim_args['optimizer'].lower() == 'adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=optim_args['lr'])\n",
    "    elif optim_args['optimizer'].lower() == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(),    lr=optim_args['lr'])\n",
    "    elif optim_args['optimizer'].lower() == 'lamb':\n",
    "        optimizer = lamb.Lamb(model.parameters(),     lr=optim_args['lr'],  betas=(0.9, 0.98), eps=1e-9, weight_decay=optim_args['weight_decay'])\n",
    "    elif optim_args['optimizer'].lower() == 'jitlamb':\n",
    "        optimizer = lamb.JITLamb(model.parameters(),  lr=optim_args['lr'],  betas=(0.9, 0.98), eps=1e-9, weight_decay=optim_args['weight_decay'])\n",
    "    elif optim_args['optimizer'].lower() == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(),   lr=optim_args['lr'], weight_decay=optim_args['weight_decay'])\n",
    "    return optimizer     \n",
    "\n",
    "class ImageDatasetWithIndex(ImageDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_files: Sequence[str],\n",
    "        seg_files: Optional[Sequence[str]] = None,\n",
    "        labels: Optional[Sequence[float]] = None,\n",
    "        transform: Optional[Callable] = None,\n",
    "        seg_transform: Optional[Callable] = None,\n",
    "        label_transform: Optional[Callable] = None,\n",
    "        image_only: bool = True,\n",
    "        transform_with_metadata: bool = False,\n",
    "        dtype: DtypeLike = np.float32,\n",
    "        reader: Optional[Union[ImageReader, str]] = None,\n",
    "        sample_indexes: Optional[Sequence[int]] = None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            image_files=image_files,\n",
    "            seg_files=seg_files,\n",
    "            labels=labels,\n",
    "            transform=transform,\n",
    "            seg_transform=seg_transform,\n",
    "            label_transform=label_transform,\n",
    "            image_only=image_only,\n",
    "            transform_with_metadata=transform_with_metadata,\n",
    "            dtype=dtype,\n",
    "            reader=reader,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.sample_indexes = sample_indexes\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = super().__getitem__(index)[:2]\n",
    "        # label = F.one_hot(torch.tensor(label), num_classes = num_classes)\n",
    "        img_path = self.image_files[index]\n",
    "        \n",
    "        if self.sample_indexes is not None:\n",
    "            sample_index = self.sample_indexes[index]\n",
    "            return image, label, sample_index\n",
    "        else:\n",
    "            return image, label\n",
    "\n",
    "def create_dataset(images, targets, sample_indexes, apply_augmentation=True):\n",
    "    \n",
    "    if apply_augmentation:\n",
    "        transform = Compose([ToTensor(),\n",
    "                             AddChannel(), \n",
    "                             SpatialPad(spatial_size=(160, 220, 350)),\n",
    "                            ])\n",
    "\n",
    "        dataset = ImageDatasetWithIndex(image_files=images, labels=targets, sample_indexes=sample_indexes, transform=transform)\n",
    "    else:\n",
    "        transform = Compose([ToTensor(),\n",
    "                             AddChannel(), \n",
    "                             SpatialPad(spatial_size=(160, 220, 350)),\n",
    "                            ])\n",
    "        dataset = ImageDatasetWithIndex(image_files=images, labels=targets, sample_indexes=sample_indexes, transform=transform)\n",
    "    return dataset\n",
    "\n",
    "def calculate_performance(outputs, targets, threshold):\n",
    "#     outputs = torch.tensor([0.1, 0.2, 0.2, 0.2, 0.2])\n",
    "#     targets = torch.tensor([1, 0, 0, 0, 0])\n",
    "    targets = targets.int()\n",
    "    acc = torchmetrics.Accuracy(task='binary', threshold = threshold)(outputs, targets)\n",
    "    f1 = torchmetrics.F1Score(task='binary', threshold = threshold)(outputs, targets)\n",
    "    sensitivity = torchmetrics.Recall(task='binary', threshold = threshold)(outputs, targets)\n",
    "    specificity = torchmetrics.Specificity(task='binary', threshold = threshold)(outputs, targets)\n",
    "    precision = torchmetrics.Precision(task='binary', threshold = threshold)(outputs, targets)\n",
    "    auprc = torchmetrics.AveragePrecision(task='binary')(outputs, targets)\n",
    "    auroc = torchmetrics.AUROC(task='binary')(outputs, targets)\n",
    "    \n",
    "    return round(float(auroc),3), round(float(auprc),3), round(float(acc),3), round(float(f1),3), round(float(sensitivity),3), round(float(specificity),3), round(float(precision),3)\n",
    "\n",
    "class AverageMeter (object):\n",
    "    def __init__(self):\n",
    "        self.reset ()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "class LossSaver(object):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    def reset(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    def update(self, train_loss, val_loss):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "    def return_list(self):\n",
    "        return self.train_losses, self.val_losses\n",
    "    def save_as_csv(self, csv_file):\n",
    "        df = pd.DataFrame({'Train Losses': self.train_losses, 'Validation Losses': self.val_losses})\n",
    "        df.index = [f\"{i+1} Epoch\" for i in df.index]\n",
    "        df.to_csv(csv_file, index=True)\n",
    "        \n",
    "\n",
    "def control_random_seed(seed, pytorch=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available()==True:\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "    except:\n",
    "        pass\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "class DualOutput:\n",
    "    def __init__(self, file, stdout):\n",
    "        self.file = file\n",
    "        self.stdout = stdout\n",
    "\n",
    "    def write(self, text):\n",
    "        self.file.write(text)\n",
    "        self.stdout.write(text)\n",
    "\n",
    "    def flush(self):\n",
    "        self.file.flush()\n",
    "        self.stdout.flush()\n",
    "def str_to_class(classname):\n",
    "    return getattr(sys.modules[__name__], classname)\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, device=False):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        alpha = self.alpha.to(inputs.device)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = (alpha[targets] * (1 - pt) ** gamma * ce_loss).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf00bb91-e6f4-46ce-820a-93ad29bae6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # print(\"args : \", args)\n",
    "    Experiments_Time, iteration, module_name, model_name, \\\n",
    "    model_dir, output_dir, batch_size, epochs, num_workers, \\\n",
    "    dataset_dir, label_file, devices, \\\n",
    "    in_channels, num_classes, optim_args, lr_scheduler_args, loss_function_args = args\n",
    "    \n",
    "    seed = iteration\n",
    "    control_random_seed(seed)                 \n",
    "    \n",
    "    device = torch.device(\"cuda:\"+str(devices[0]))\n",
    "    \n",
    "    module = importlib.import_module(f'models.{module_name}')\n",
    "    model = str_to_class(model_name)(in_channels, num_classes)\n",
    "    try:\n",
    "        init_weights(model)    \n",
    "    except:\n",
    "        pass\n",
    "    if len(devices)>1:\n",
    "        model = torch.nn.DataParallel(model, device_ids = devices ).to(device)\n",
    "    else:\n",
    "        model = model.to(device)\n",
    "        \n",
    "    optimizer = configure_optimizer(model, optim_args)\n",
    "    if lr_scheduler_args['lr_scheduler'] == 'CosineAnnealingWarmRestarts':\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = lr_scheduler_args['T_0'], eta_min = lr_scheduler_args['eta_min'])\n",
    "    elif lr_scheduler_args['lr_scheduler'] == 'CosineAnnealingLR':\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = lr_scheduler_args['T_max'], eta_min = lr_scheduler_args['eta_min'])\n",
    "    \n",
    "\n",
    "    df_label = pd.read_csv(label_file, header=None, index_col=0) #[:100]\n",
    "\n",
    "    images = [os.path.join(dataset_dir, f'{i}') for i in df_label.index.to_list()]\n",
    "    labels = df_label.to_numpy(dtype=np.int64).flatten()\n",
    "   \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_test_split_ratio = 0.8\n",
    "    train_val_split_ratio = 0.75\n",
    "    train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "        images, labels, train_size=train_test_split_ratio, random_state=seed\n",
    "    )\n",
    "    train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "        train_images, train_labels, train_size=train_val_split_ratio, random_state=seed,\n",
    "        stratify=train_labels\n",
    "    )\n",
    "    from collections import Counter\n",
    "      \n",
    "    train_class_ratios = {k: round(v / len(train_labels), 3) for k, v in Counter(train_labels).items()}\n",
    "    val_class_ratios = {k: round(v / len(val_labels), 3) for k, v in Counter(val_labels).items()}\n",
    "    test_class_ratios = {k: round(v / len(test_labels), 3) for k, v in Counter(test_labels).items()}\n",
    "    \n",
    "    print(f\"Class ratios: {train_class_ratios} / {val_class_ratios} / {test_class_ratios}\")\n",
    "    \n",
    "    train_indexes = [(train_images + val_images + test_images).index(x) for x in train_images]; val_indexes = [(train_images + val_images + test_images).index(x) for x in val_images]; test_indexes = [(train_images + val_images + test_images).index(x) for x in test_images]\n",
    "\n",
    "    df_data_split = pd.DataFrame({'Images': [os.path.basename(image) for image in train_images + val_images + test_images], 'Labels': list(train_labels) + list(val_labels) + list(test_labels)})\n",
    "    df_data_split = df_data_split.reset_index(drop=True).reset_index().rename(columns={'index': 'Sample Index'})\n",
    "    data_split = (\n",
    "        ['Train'] * len(train_images) +\n",
    "        ['Validation'] * len(val_images) +\n",
    "        ['Test'] * len(test_images)\n",
    "    )\n",
    "    df_data_split['Data Split'] = data_split\n",
    "        \n",
    "    trainset = create_dataset(train_images, train_labels, train_indexes)\n",
    "    validset = create_dataset(val_images, val_labels, val_indexes, apply_augmentation=False)\n",
    "    testset = create_dataset(test_images, test_labels, test_indexes, apply_augmentation=False)\n",
    "    print(f'Data Split (Train/Val/Test): {len(train_images)}/{len(val_images)}/{len(test_images)}')\n",
    "    \n",
    "    train_sampler = None\n",
    "    valid_sampler = None\n",
    "    test_sampler  = None\n",
    "    suffle = False\n",
    "    \n",
    "    train_loader = DataLoader(trainset, num_workers=num_workers, shuffle=False,\n",
    "                          sampler=train_sampler, batch_size=batch_size ,\n",
    "                          pin_memory=False, drop_last=False,\n",
    "                          collate_fn=None)\n",
    "    \n",
    "    valid_loader = DataLoader(validset, num_workers=num_workers, shuffle=False,\n",
    "                          sampler=valid_sampler, batch_size=batch_size ,\n",
    "                          pin_memory=False, drop_last=False,\n",
    "                          collate_fn=None)\n",
    "    \n",
    "    test_loader = DataLoader(testset, num_workers=num_workers, shuffle=False,\n",
    "                          sampler=test_sampler, batch_size=batch_size,\n",
    "                          pin_memory=False, drop_last=False,\n",
    "                          collate_fn=None)\n",
    "\n",
    "    if loss_function_args['loss_function'] == 'BCEWithLogitsLoss':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif loss_function_args['loss_function'] == 'FocalLoss':\n",
    "        class_counts = np.bincount(train_labels)\n",
    "        num_classes = len(class_counts)\n",
    "        total_samples = len(train_labels)\n",
    "        class_weights = []\n",
    "        for count in class_counts:\n",
    "            weight = 1 / (count / total_samples)\n",
    "            class_weights.append(weight)\n",
    "        class_weights = torch.FloatTensor(class_weights)\n",
    "        criterion = FocalLoss(alpha=class_weights, gamma=loss_function_args['gamma'])\n",
    "    elif loss_function_args['loss_function'] == 'CrossEntropyLoss':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    loss_saver = LossSaver()\n",
    "    Best_Loss = 999999999999999\n",
    "    Best_Epoch = 1\n",
    "    now = datetime.now()\n",
    "    Train_Time = now.strftime(\"%y%m%d_%H%M%S\")\n",
    "    start = timeit.default_timer()\n",
    "    print(f'Train Start ({Train_Time})')\n",
    "    file_path = os.path.join(output_dir, f'{Train_Time}_{model_name}_iter_{iteration}.pt')\n",
    "    save_checkpoint(args, model, optimizer, optim_args, 0, Best_Loss, output_dir, file_path)\n",
    "\n",
    "    Early_Stop = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss =  train_epoch(model, optimizer, criterion, train_loader, device)\n",
    "        lr_scheduler.step()\n",
    "        outputs, targets, sample_indexes = infer(model, criterion, valid_loader, device)\n",
    "        val_loss = round(float(criterion(outputs, targets).cpu().numpy()),6)\n",
    "        now = datetime.now()\n",
    "        infer_date = now.strftime(\"%y%m%d_%H%M%S\")        \n",
    "        print(f'{epoch} EP({infer_date}): Loss: train-{train_loss}/val-{val_loss}',end=' ')\n",
    "        loss_saver.update(train_loss, val_loss)\n",
    "        loss_saver.save_as_csv(f'output/output_{Experiments_Time}/{model_name}_iter_{seed}/Losses_{Experiments_Time}.csv')\n",
    "            \n",
    "        if Best_Loss >= val_loss:\n",
    "            save_checkpoint(args, model, optimizer, optim_args, epoch, val_loss, output_dir, file_path)\n",
    "            Best_Loss = val_loss\n",
    "            Best_Epoch = epoch\n",
    "            Early_Stop = 0\n",
    "            print(f\"Best Epoch: {epoch}, Loss: {val_loss}\", end=' ')\n",
    "            threshold_range = [0, 1]\n",
    "            graduation = (threshold_range[1]-threshold_range[0])/1000\n",
    "            threshold_list=list(np.round(np.linspace(threshold_range[0]+graduation,threshold_range[1]-graduation,999),3))\n",
    "            min_diff = 9999\n",
    "            THRESHOLD = None\n",
    "            outputs = outputs[:,1]\n",
    "            for threshold in threshold_list:\n",
    "                ss = torchmetrics.Recall(task='binary', threshold = threshold)(outputs, targets)\n",
    "                sp = torchmetrics.Specificity(task='binary', threshold = threshold)(outputs, targets)\n",
    "                if min_diff >= np.abs(ss-sp):\n",
    "                    min_diff = np.abs(ss-sp)\n",
    "                    THRESHOLD = threshold\n",
    "            print(f'(Cutoff: {THRESHOLD}) Loss: {val_loss}')\n",
    "            df_data_split['Model Validation Output'] = 'No Validation'; df_data_split['Model Validation Target'] = 'No Validation';\n",
    "            for i in range(outputs.shape[0]):\n",
    "                df_data_split.loc[df_data_split['Sample Index'] == int(sample_indexes[i]), 'Model Validation Output'] = np.round(float(outputs[i]),6)\n",
    "                df_data_split.loc[df_data_split['Sample Index'] == int(sample_indexes[i]), 'Model Validation Target'] = 1 if float(outputs[i])>= THRESHOLD else 0\n",
    "            df_data_split.to_csv(f'output/output_{Experiments_Time}/{model_name}_iter_{seed}/Data_split_and_Outputs_Targets_{Experiments_Time}.csv', index=False)\n",
    "        else:\n",
    "            print('')\n",
    "            Early_Stop+=1\n",
    "        if Early_Stop>=20:\n",
    "            break\n",
    "    stop = timeit.default_timer()\n",
    "    training_time = round((stop - start),2)\n",
    "    print(f\"Train End ({datetime.now().strftime('%y%m%d_%H%M%S')})\")\n",
    "    \n",
    "    # validation for threshold\n",
    "    print(f'Validation Start ({datetime.now().strftime(\"%y%m%d_%H%M%S\")})')\n",
    "    load_checkpoint(model, optimizer, file_path)\n",
    "    outputs, targets, sample_indexes = infer(model, criterion, valid_loader, device)\n",
    "    val_loss = round(float(criterion(outputs, targets).cpu().numpy()),6)\n",
    "    threshold_range = [0, 1]\n",
    "    graduation = (threshold_range[1]-threshold_range[0])/100000\n",
    "    threshold_list=list(np.round(np.linspace(threshold_range[0]+graduation,threshold_range[1]-graduation,99999),5))\n",
    "    min_diff = 9999\n",
    "    THRESHOLD = None\n",
    "    outputs = outputs[:,1]\n",
    "    for threshold in threshold_list:\n",
    "        ss = torchmetrics.Recall(task='binary', threshold = threshold)(outputs, targets)\n",
    "        sp = torchmetrics.Specificity(task='binary', threshold = threshold)(outputs, targets)\n",
    "        if min_diff >= np.abs(ss-sp):\n",
    "            min_diff = np.abs(ss-sp)\n",
    "            THRESHOLD = threshold\n",
    "    print(f'(Cutoff: {THRESHOLD}) Loss: {val_loss}')\n",
    "    df_data_split['Model Validation Output'] = 'No Validation'; df_data_split['Model Validation Target'] = 'No Validation';\n",
    "    for i in range(outputs.shape[0]):\n",
    "        df_data_split.loc[df_data_split['Sample Index'] == int(sample_indexes[i]), 'Model Validation Output'] = np.round(float(outputs[i]),6)\n",
    "        df_data_split.loc[df_data_split['Sample Index'] == int(sample_indexes[i]), 'Model Validation Target'] = 1 if float(outputs[i])>= THRESHOLD else 0\n",
    "    df_data_split.to_csv(f'output/output_{Experiments_Time}/{model_name}_iter_{seed}/Data_split_and_Outputs_Targets_{Experiments_Time}.csv', index=False)\n",
    "    print(f'Validation End ({datetime.now().strftime(\"%y%m%d_%H%M%S\")})')\n",
    "    \n",
    "    # test\n",
    "    now = datetime.now()\n",
    "    Test_Time = now.strftime(\"%y%m%d_%H%M%S\")\n",
    "    print(f\"Test Start ({Test_Time})\")\n",
    "    load_checkpoint(model, optimizer, file_path)\n",
    "    start = timeit.default_timer()\n",
    "          \n",
    "    outputs, targets, sample_indexes = infer(model, criterion, test_loader, device)\n",
    "    \n",
    "    loss = round(float(criterion(outputs, targets).cpu().numpy()),6)\n",
    "    outputs = outputs[:,1]\n",
    "    auroc, auprc, acc, f1, ss, sp, pr = calculate_performance(outputs, targets, THRESHOLD)\n",
    "    \n",
    "    print(f'Test({datetime.now().strftime(\"%y%m%d_%H%M%S\")}): Loss: {loss}, AUROC: {auroc}, AUPRC: {auprc}, ACC: {acc}, F1: {f1}, SS: {ss}, SP: {sp}, PR:{pr}')\n",
    "    params = count_parameters(model)\n",
    "    stop = timeit.default_timer()\n",
    "    test_time = round((stop - start),2)\n",
    "\n",
    "    Performances = [Experiments_Time, Train_Time, seed, model_name, loss, auroc, auprc, acc, f1, ss, sp, pr, THRESHOLD, params, training_time, test_time, Best_Epoch, os.getcwd()]\n",
    "    df = pd.read_csv(f'output/output_{Experiments_Time}/Performance_{Experiments_Time}.csv')\n",
    "    df = pd.concat([df, pd.DataFrame([Performances], columns = df.columns)], ignore_index=True)\n",
    "    df.to_csv(f'output/output_{Experiments_Time}/Performance_{Experiments_Time}.csv', index=False, header=True)\n",
    "    df_data_split['Model Test Output'] = 'No Test'; df_data_split['Model Test Target'] = 'No Test';\n",
    "    for i in range(outputs.shape[0]):\n",
    "        df_data_split.loc[df_data_split['Sample Index'] == int(sample_indexes[i]), 'Model Test Output'] = np.round(float(outputs[i]),6)\n",
    "        df_data_split.loc[df_data_split['Sample Index'] == int(sample_indexes[i]), 'Model Test Target'] = 1 if float(outputs[i])>= THRESHOLD else 0\n",
    "    df_data_split.to_csv(f'output/output_{Experiments_Time}/{model_name}_iter_{seed}/Data_split_and_Outputs_Targets_{Experiments_Time}.csv', index=False)\n",
    "\n",
    "    print(f\"Test End ({datetime.now().strftime('%y%m%d_%H%M%S')})\")\n",
    "    \n",
    "Experiments_Time = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "output_root = f'output/output_{Experiments_Time}'\n",
    "os.makedirs(output_root, exist_ok = True)\n",
    "df = pd.DataFrame(index=None, columns=Metrics)\n",
    "df.to_csv(f'output/output_{Experiments_Time}/Performance_{Experiments_Time}.csv', index=False, header=True)\n",
    "for iteration in range(iterations[0], iterations[1]+1):\n",
    "    for i, (module_name, model_name) in enumerate(zip(module_names, model_names)):\n",
    "        print(f'{model_name} (iter: {iteration})')\n",
    "        if (model_name=='Xuefei_Song_ResNet18_Proposed_v8_18') and iteration<4:\n",
    "            continue\n",
    "        output_dir = output_root + f'/{model_name}_iter_{iteration}'\n",
    "        copy_sourcefile(output_dir, src_dir='src')\n",
    "        if save_log == True:\n",
    "            original_stdout = sys.stdout\n",
    "            log_file = open(f'{output_dir}/Log.txt', 'w')\n",
    "            sys.stdout = DualOutput(log_file, original_stdout)\n",
    "        args = [\n",
    "            Experiments_Time, iteration, module_name, model_name, \n",
    "            model_dir, output_dir, batch_size, epochs, num_workers,            \n",
    "            dataset_dir, label_file, devices,\n",
    "            in_channels, num_classes, optim_args, lr_scheduler_args, loss_function_args\n",
    "            ]\n",
    "        main(args)\n",
    "        if save_log == True:\n",
    "            sys.stdout = original_stdout\n",
    "            log_file.close()\n",
    "        copy_sourcefile(output_dir, src_dir='src')\n",
    "import os\n",
    "print('End')\n",
    "os._exit(00)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSH_Torch_2.0",
   "language": "python",
   "name": "lsh_torch_2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
